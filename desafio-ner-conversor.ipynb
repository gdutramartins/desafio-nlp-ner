{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafio NER - Conversão dos Arquivos IOB para JSON e Spacy\n",
    "* Agrupa por Label ou por tipo de Dataset (Treino, Validação e Teste). \n",
    "* Caso deseje agrupar treino com validação (train_dev) basta ajustar o vetor DATASET_TYPE com a constante TRAIN_DEV_DATASET.\n",
    "* Junção de tokens com espaço em branco.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.training import Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_DRUG_PROTEIN = 'DRUG-PROTEIN'\n",
    "LABEL_CHEMICAL = 'CHEMICAL'\n",
    "LABEL_DISEASE = 'DISEASE'\n",
    "LABEL_SPECIES = 'SPECIES'\n",
    "\n",
    "LABEL_LIST = [LABEL_DRUG_PROTEIN,\n",
    "              LABEL_CHEMICAL,\n",
    "              LABEL_DISEASE,    \n",
    "              LABEL_SPECIES]\n",
    "\n",
    "LABEL_TO_DIR = {\n",
    "    LABEL_DRUG_PROTEIN: ['BC2GM', 'JNLPBA'],\n",
    "    LABEL_CHEMICAL: ['BC4CHEMD','BC5CDR-chem'],\n",
    "    LABEL_DISEASE: ['BC5CDR-disease', 'NCBI-disease'],    \n",
    "    LABEL_SPECIES: ['linnaeus', 's800']\n",
    "}\n",
    "\n",
    "DATA_ORIGIN_PATH = os.path.join(\"data\",\"origin\")\n",
    "DATA_PREPARED_PATH = os.path.join(\"data\", \"prepared\")\n",
    "DATA_AGGREGATE_PATH = os.path.join(DATA_PREPARED_PATH, \"aggregate\")\n",
    "\n",
    "WORD_VECTOR_PATH = \"word_vec\"\n",
    "WORD_VECTOR_MODEL_NAME = os.path.join(WORD_VECTOR_PATH,\"biomed.model\")\n",
    "WORD_VECTOR_FILE_NAME = os.path.join(WORD_VECTOR_PATH,\"biomed_word2vec.txt\")\n",
    "WORD_VECTOR_GLOVE = os.path.join(WORD_VECTOR_PATH,\"glove.840B.300d.txt\")\n",
    "WORD_VECTOR_PUBMED_PMC_ORIGINAL = os.path.join(WORD_VECTOR_PATH, \"ri-3gram-400-tsv\", \"vectors.tsv\")\n",
    "VOCAB_PUBMED_PMC = os.path.join(WORD_VECTOR_PATH, \"ri-3gram-400-tsv\", \"vocab.tsv\")\n",
    "WORD_VECTOR_PUBMED_PMC_PREPARED = os.path.join(WORD_VECTOR_PATH, \"pubmed_pmc_word2vec.txt\")\n",
    "\n",
    "\n",
    "MODEL_PATH = \"model\"\n",
    "MODEL_TRAIN_PATH = os.path.join(MODEL_PATH, \"train\")\n",
    "MODEL_ACTUAL_PATH = os.path.join(MODEL_PATH, \"actual\")\n",
    "\n",
    "TSV_EXTENSION = \".tsv\"\n",
    "JSON_EXTENSION = \".json\"\n",
    "SPACY_EXTENSION = \".spacy\"\n",
    "\n",
    "AUG_SUFFIXE = \"_aug\"\n",
    "\n",
    "TRAIN_DEV_DATASET = \"train_dev\"\n",
    "TRAIN_DATASET = \"train\"\n",
    "TRAIN_AUGMENT_DATASET = \"train_aug\"\n",
    "VALIDATE_DATASET = \"devel\"\n",
    "TEST_DATASET = \"test\"\n",
    "TRAIN_AUG_VALIDATE_DATASET = \"train_dev_aug\"\n",
    "\n",
    "DATASET_TYPE = [TRAIN_DATASET, VALIDATE_DATASET, TEST_DATASET]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversão arquivo IOB para os formatos JSON(formato próprio) e SPACY (para treinamento em linha de comando)\n",
    "#### Json gerado com formato próprio que será tratado no momento do treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processa_iob_dataset(lista_annotations, label, dataset_type, dir_dataset, to_lower):\n",
    "    entidade_atual = \"\"\n",
    "    ini_entidade_atual = -1\n",
    "    entities = []\n",
    "    sentenca = \"\"\n",
    "    \n",
    "    dataset_iob_file = os.path.join(DATA_ORIGIN_PATH, dir_dataset, dataset_type + TSV_EXTENSION)\n",
    "    with open(dataset_iob_file) as f_iob:\n",
    "        for linha in f_iob:\n",
    "            if len(entidade_atual) > 0 and (\"\\tO\" in linha or \"\\tB\" in linha or linha == \"\\n\"):\n",
    "                entities.append({\"entidade\":entidade_atual, \n",
    "                                     \"start\":ini_entity_atual, \n",
    "                                     \"end\": ini_entity_atual + len(entidade_atual),\n",
    "                                     \"label\": label\n",
    "                                    })\n",
    "                entidade_atual = \"\"\n",
    "                ini_entity_atual = -1\n",
    "\n",
    "            if linha != \"\\n\":\n",
    "                if (len(sentenca) != 0):\n",
    "                    sentenca += \" \"\n",
    "                if len(entidade_atual) > 0:\n",
    "                    entidade_atual += \" \"\n",
    "\n",
    "                if (\"\\tO\" in linha):\n",
    "                    linha_tratada = linha.replace(\"\\tO\",\"\").replace(\"\\n\", \"\")\n",
    "                elif(\"\\tB\" in linha):\n",
    "                    ini_entity_atual = len(sentenca)\n",
    "                    linha_tratada = linha.replace(\"\\tB\",\"\").replace(\"\\n\", \"\")\n",
    "                    entidade_atual = linha_tratada                \n",
    "                elif(\"\\tI\" in linha):\n",
    "                    linha_tratada = linha.replace(\"\\tI\",\"\").replace(\"\\n\", \"\")\n",
    "                    entidade_atual += linha_tratada\n",
    "                \n",
    "                if to_lower:\n",
    "                    linha_tratada = linha_tratada.to_lower()\n",
    "                sentenca = sentenca + linha_tratada\n",
    "            else:\n",
    "                lista_annotations.append({\"texto\": sentenca, \"entities\": entities})\n",
    "                sentenca = \"\"\n",
    "                entities=[]\n",
    "    return lista_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path - caminho que será gravado, sem o nome do arquivo\n",
    "# file_name - nome do arquivo sem extensão\n",
    "def save_converted_file(lista_annotations, path, file_name, save_json, save_spacy):\n",
    "    if save_json:\n",
    "        json_file = os.path.join(path, file_name + JSON_EXTENSION)\n",
    "        if os.path.exists(json_file):\n",
    "            os.remove(json_file)\n",
    "        Path(path).mkdir(parents=True, exist_ok=True)\n",
    "        with open(json_file, 'w') as json_file:            \n",
    "            json.dump(lista_annotations, json_file)\n",
    "\n",
    "    if save_spacy:\n",
    "        nlp = spacy.blank(\"en\") # load a new spacy model\n",
    "        db = DocBin() # create a DocBin object\n",
    "        for an in lista_annotations:\n",
    "            doc = nlp.make_doc(an['texto']) # create doc object from text\n",
    "            ents=[]\n",
    "            for entidade in an['entities']:\n",
    "                span = doc.char_span(entidade['start'], entidade['end'], label=entidade['label'], alignment_mode=\"contract\")\n",
    "                if span is None:\n",
    "                    print (\"Span None\")\n",
    "                    print(ner['texto'])\n",
    "                    print(entidade)\n",
    "                else:\n",
    "                    ents.append(span)\n",
    "\n",
    "            doc.ents = ents\n",
    "            db.add(doc)\n",
    "\n",
    "        spacy_file = os.path.join(path, file_name + SPACY_EXTENSION)\n",
    "        if os.path.exists(spacy_file):\n",
    "            os.remove(spacy_file)\n",
    "        db.to_disk(spacy_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_IOB_json_spacy(conv_json=True, conv_spacy=True, group_by_label=True, to_lower=False):\n",
    "    \n",
    "    for dataset_type in DATASET_TYPE:\n",
    "        print(dataset_type)\n",
    "        if not group_by_label:\n",
    "            lista_annotations = []\n",
    "        for label in LABEL_LIST:\n",
    "            if (group_by_label):\n",
    "                lista_annotations = []\n",
    "    \n",
    "            for dir_dataset in LABEL_TO_DIR[label]:\n",
    "                lista_annotations = processa_iob_dataset(lista_annotations, label, dataset_type, dir_dataset, to_lower)\n",
    "\n",
    "            if group_by_label:\n",
    "                path = os.path.join(DATA_PREPARED_PATH, label)\n",
    "                file = label + \"-\" + dataset_type\n",
    "                save_converted_file(lista_annotations, path, file, save_json=conv_json, save_spacy=conv_spacy)\n",
    "        if not group_by_label:\n",
    "            file = dataset_type\n",
    "            save_converted_file(lista_annotations, DATA_AGGREGATE_PATH, file, save_json=conv_json, save_spacy=conv_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converte IOB para JSON agrupando por Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "devel\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "convert_IOB_json_spacy(conv_json=True, conv_spacy=False, group_by_label=True, to_lower=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converte IOB para JSON agrupando por Dataset (Train, Dev e Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_IOB_json_spacy(conv_json=True, conv_spacy=False, group_by_label=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratamento de Entidades com Caracteres Especiais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Gerar mais exemplos quando existirem caracteres especiais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trata_sentenca_iob(lista_iob_entries):\n",
    "    precisa_nova_sentenca = False\n",
    "    nova_sentenca = []\n",
    "    pos = 0\n",
    "    \n",
    "    token_anterior, tipo_token_anterior = None, None\n",
    "    \n",
    "    while pos < len(lista_iob_entries):\n",
    "        entry = lista_iob_entries[pos]\n",
    "        entry = entry.replace(\"\\n\",\"\")\n",
    "        split = entry.split(\"\\t\")\n",
    "        token = split[0]\n",
    "        tipo_token = split[1].strip()\n",
    "        \n",
    "        if tipo_token == \"O\":            \n",
    "            pos+= 1\n",
    "            token_anterior, tipo_token_anterior = None, None\n",
    "            nova_sentenca.append(entry)\n",
    "            continue\n",
    "        elif len(token) > 1:\n",
    "            nova_sentenca.append(entry)\n",
    "        elif len(token) == 1:\n",
    "            if token in \"-_:/\\\\\" and token_anterior != None and pos < len(lista_iob_entries) - 1: #junta anterior + atual + posterior\n",
    "                # existe token anterior adiciona, deve ser removido porque será concatenado\n",
    "                del nova_sentenca[-1]\n",
    "                \n",
    "                token_proximo = lista_iob_entries[pos+1].split(\"\\t\")[0] \n",
    "                tipo_token_proximo = lista_iob_entries[pos+1].split(\"\\t\")[1].strip()\n",
    "                if tipo_token_proximo != \"O\":\n",
    "                    precisa_nova_sentenca = True\n",
    "                    pos+=1\n",
    "                    novo_token = token_anterior + token + token_proximo\n",
    "                    nova_sentenca.append(novo_token + \"\\t\" + tipo_token_anterior)                    \n",
    "                else:\n",
    "                    nova_sentenca.append(entry)\n",
    "            \n",
    "            elif token in \".%+)]}\" and token_anterior != None: #junta com o anterior\n",
    "                precisa_nova_sentenca = True\n",
    "                # existe token anterior adiciona, deve ser removido porque será concatenado\n",
    "                del nova_sentenca[-1]\n",
    "                                \n",
    "                novo_token = token_anterior + token\n",
    "                nova_sentenca.append(novo_token + \"\\t\" + tipo_token_anterior)                    \n",
    "            \n",
    "            elif token in \"([{\" and pos < len(lista_iob_entries) - 1 : #junta com o posterior\n",
    "                \n",
    "                token_proximo = lista_iob_entries[pos+1].split(\"\\t\")[0] \n",
    "                tipo_token_proximo = lista_iob_entries[pos+1].split(\"\\t\")[1].strip()\n",
    "                if tipo_token_proximo != \"O\":\n",
    "                    precisa_nova_sentenca = True\n",
    "                    pos+=1\n",
    "                    novo_token = token + token_proximo\n",
    "                    nova_sentenca.append(novo_token + \"\\t\" + tipo_token)                    \n",
    "                else:\n",
    "                    nova_sentenca.append(entry)\n",
    "            \n",
    "            elif token in \"\\\"'*$\": #remove o token                \n",
    "                precisa_nova_sentenca = True                                \n",
    "            else:\n",
    "                nova_sentenca.append(entry)\n",
    "        #if \"sulfonatopropoxyl\" in \" \".join([n for n in nova_sentenca]):\n",
    "            #print(nova_sentenca)\n",
    "        # tratamento de erro\n",
    "        if len(nova_sentenca) == 0:\n",
    "            print(lista_iob_entries)\n",
    "            print(token, tipo_token)\n",
    "        token_anterior = nova_sentenca[-1].split(\"\\t\")[0]                 \n",
    "        tipo_token_anterior = nova_sentenca[-1].split(\"\\t\")[1].strip()\n",
    "        pos+= 1        \n",
    "        \n",
    "    if precisa_nova_sentenca:\n",
    "        return nova_sentenca\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def augment_iob_file(dir_dataset, label, dataset_type):\n",
    "    dataset_iob_file = os.path.join(DATA_ORIGIN_PATH, dir_dataset, dataset_type + TSV_EXTENSION)\n",
    "    lista_novas_sentencas = []\n",
    "    with open(dataset_iob_file,\"r\") as f_iob:\n",
    "        lista_iob_entries =[]\n",
    "        for linha in f_iob:\n",
    "            if linha != \"\\n\":\n",
    "                lista_iob_entries.append(linha)\n",
    "            else:\n",
    "                nova_sentenca = trata_sentenca_iob(lista_iob_entries)\n",
    "                if nova_sentenca != None:\n",
    "                    lista_novas_sentencas.append(nova_sentenca)\n",
    "                lista_iob_entries = []\n",
    "    if len(lista_novas_sentencas) > 0:\n",
    "        dataset_augment_file = os.path.join(DATA_ORIGIN_PATH, dir_dataset, dataset_type + AUG_SUFFIXE + TSV_EXTENSION)\n",
    "        with open(dataset_augment_file,\"w\") as f_aug:\n",
    "            for nova_sentenca in lista_novas_sentencas:\n",
    "                for item in nova_sentenca:\n",
    "                    f_aug.write(item)\n",
    "                    f_aug.write(\"\\n\")\n",
    "                f_aug.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def augment_train_datasets():\n",
    "    for label in LABEL_LIST:\n",
    "        for dataset_type in [TRAIN_DATASET, TRAIN_DEV_DATASET]:\n",
    "            for dir_dataset in LABEL_TO_DIR[label]:\n",
    "                augment_iob_file(dir_dataset, label, dataset_type)\n",
    "\n",
    "augment_train_datasets()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
